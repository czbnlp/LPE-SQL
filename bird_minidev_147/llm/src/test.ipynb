{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取zero-shot的结果\n",
    "from utils import save_data\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def package_sqls(\n",
    "    sql_path, db_root_path, engine, sql_dialect=\"SQLite\", mode=\"gpt\", data_mode=\"dev\"\n",
    "):\n",
    "    clean_sqls = []\n",
    "    db_path_list = []\n",
    "    if mode == \"gpt\":\n",
    "        # use chain of thought\n",
    "        sql_data = json.load(path)\n",
    "        for _, sql_str in sql_data.items():\n",
    "            if type(sql_str) == str:\n",
    "                sql, db_name = sql_str.split(\"\\t----- bird -----\\t\")\n",
    "            else:\n",
    "                sql, db_name = \" \", \"financial\"\n",
    "            clean_sqls.append(sql)\n",
    "            db_path_list.append(db_root_path + db_name + \"/\" + db_name + \".sqlite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_json(dir):\n",
    "    with open(dir, \"r\") as j:\n",
    "        contents = json.loads(j.read())\n",
    "    return contents\n",
    "\n",
    "def compute_acc_by_diff(exec_results, diff_json_path, results_path, metric=\"EX\"):\n",
    "    num_queries = len(exec_results)\n",
    "    contents = load_json(diff_json_path)\n",
    "    simple_results, moderate_results, challenging_results = [], [], []\n",
    "    \n",
    "    batch_size = 10\n",
    "    for i, content in enumerate(contents):\n",
    "        if content[\"difficulty\"] == \"simple\":\n",
    "            simple_results.append(exec_results[i])\n",
    "\n",
    "        if content[\"difficulty\"] == \"moderate\":\n",
    "            moderate_results.append(exec_results[i])\n",
    "\n",
    "        if content[\"difficulty\"] == \"challenging\":\n",
    "            try:\n",
    "                challenging_results.append(exec_results[i])\n",
    "            except:\n",
    "                print(i)\n",
    "\n",
    "        # Process every batch_size examples\n",
    "        if (i + 1) % batch_size == 0 or i + 1 == num_queries:\n",
    "            simple_acc = sum([res[\"res\"] for res in simple_results]) / len(simple_results) if simple_results else 0\n",
    "            moderate_acc = sum([res[\"res\"] for res in moderate_results]) / len(moderate_results) if moderate_results else 0\n",
    "            challenging_acc = sum([res[\"res\"] for res in challenging_results]) / len(challenging_results) if challenging_results else 0\n",
    "            all_acc = sum([res[\"res\"] for res in exec_results[:i+1]]) / (i + 1)\n",
    "\n",
    "            count_lists = [\n",
    "                len(simple_results),\n",
    "                len(moderate_results),\n",
    "                len(challenging_results),\n",
    "                i + 1,\n",
    "            ]\n",
    "\n",
    "            score_lists = [\n",
    "                simple_acc * 100,\n",
    "                moderate_acc * 100,\n",
    "                challenging_acc * 100,\n",
    "                all_acc * 100,\n",
    "            ]\n",
    "\n",
    "            save_data(i + 1, results_path, score_lists, count_lists, metric)\n",
    "\n",
    "def save_data(idx, results_path, score_lists, count_lists, metric):\n",
    "    levels = [\"simple\", \"moderate\", \"challenging\", \"total\"]\n",
    "    \n",
    "    with open(results_path, 'a') as file:\n",
    "        file.write(f\"Result Index: {idx}\\n\")\n",
    "        file.write(\"{:20} {:20} {:20} {:20} {:20}\\n\".format(\"\", *levels))\n",
    "        file.write(\"{:20} {:<20} {:<20} {:<20} {:<20}\\n\".format(\"count\", *count_lists))\n",
    "\n",
    "        file.write(\n",
    "            f\"======================================    {metric}    =====================================\\n\"\n",
    "        )\n",
    "\n",
    "        formatted_scores = [\n",
    "            f\"{score:.2f}\" if score is not None else \"N/A\" for score in score_lists\n",
    "        ]\n",
    "        file.write(\"{:20} {:<20} {:<20} {:<20} {:<20}\\n\".format(metric, *formatted_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m mistake_examples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m mistake_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m      4\u001b[0m     [\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     ]\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m mistake_prompt\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mistake_examples = []\n",
    "\n",
    "mistake_prompt = '\\n\\n'.join(\n",
    "    [\n",
    "        f\"example{index+1}: {{\\n\" + \n",
    "        '\\n'.join([f\"{key}: {value}\" for key, value in example.items() if key != 'difficulty']) + \n",
    "        \"\\n}\"\n",
    "        for index, example in enumerate(mistake_examples)\n",
    "    ]\n",
    ")\n",
    "assert mistake_prompt!=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m predicted_sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m predicted_sql \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predicted_sql = \"\"\n",
    "assert predicted_sql != \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "                base_url=\"http://localhost:28083/v1/\",\n",
    "                api_key=\"Empty\",\n",
    "            )\n",
    "instruction = \"Hi. \"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"{0}：{1}\".format(instruction,input)},\n",
    "]\n",
    "       \n",
    "completion = client.chat.completions.create(\n",
    "    model=\"Qwen2-57B-A14B-Instruct-GPTQ-Int4\",\n",
    "    messages=messages,\n",
    "    stream=False, #设置是否流式输出\n",
    "    #max_tokens=1000,\n",
    "    top_p=1\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
